{"cells":[{"cell_type":"markdown","id":"f745aaa9","metadata":{"id":"f745aaa9"},"source":["##  Gradient Descent with Backtracking Line Search:\n","\n","\n"]},{"cell_type":"code","execution_count":1,"id":"d4ae89c9-5122-4e0c-80b9-e0973369c46e","metadata":{"tags":[],"id":"d4ae89c9-5122-4e0c-80b9-e0973369c46e","executionInfo":{"status":"ok","timestamp":1739112850899,"user_tz":-420,"elapsed":14,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["# Imports\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from numpy import log\n","import shutil\n","import sys\n","import os.path"]},{"cell_type":"markdown","id":"a78b872d","metadata":{"id":"a78b872d"},"source":["Given function:\n","\\begin{align*}\n","    f(x_1, x_2, x_3) = x_{3} \\log \\Big( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}} \\Big) + (x_{3}-2)^2 + e^{\\frac{1}{x_{1} + x_{2}}}\n","\\end{align*}\n","\n","$ \\textbf{dom} \\; f: \\{ \\mathbf{x} \\in \\mathbb{R}^3 : x_1 +x _2 >0, x_3 > 0 \\}  $"]},{"cell_type":"code","execution_count":2,"id":"6221d128-d77c-4cca-8ac4-9b32b83a2255","metadata":{"tags":[],"id":"6221d128-d77c-4cca-8ac4-9b32b83a2255","executionInfo":{"status":"ok","timestamp":1739112868688,"user_tz":-420,"elapsed":9,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["# Defining our function\n","def my_f(x):\n","    val = x[2] * log(np.exp(x[0] / x[2]) + np.exp(x[1] / x[2])) + (x[2] - 2)**2 + np.exp(1/(x[0] + x[1]))\n","    return val"]},{"cell_type":"markdown","id":"4f1c8d40","metadata":{"id":"4f1c8d40"},"source":["Defining the first derivative:\n","\n","$\\nabla f = [ \\partial f/\\partial x_1 \\; \\partial f/\\partial x_2 \\; \\partial f/\\partial x_3]^T   $\n","\n","$$ \\implies \\nabla f = \\begin{Bmatrix}\n","\\frac{e^{\\frac{x_{1}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2}  \\\\ \\\\\n","\\frac{e^{\\frac{x_{2}} {x_{3}}}}{e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}} - \\frac{e^{ \\frac{1}{x_1 + x_2}}}{(x_1 +x_2)^2} \\\\ \\\\\n"," log(e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) - \\frac{x_1 e^{\\frac{x_{1}} {x_{3}}} + x_2 e^{\\frac{x_{2}} {x_{3}}}}{x_3 ( e^{\\frac{x_{1}} {x_{3}}}+ e^{\\frac{x_{2}} {x_{3}}}) } + 2(x_3-2)\n","\\end{Bmatrix}$$"]},{"cell_type":"code","execution_count":3,"id":"117db820-5562-4313-af26-846be09c006b","metadata":{"tags":[],"id":"117db820-5562-4313-af26-846be09c006b","executionInfo":{"status":"ok","timestamp":1739112877848,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["# Defining the first derivative of the function\n","def nabla_f(x):\n","    x1, x2, x3 = x[0], x[1], x[2]\n","    f = np.array([\n","        [np.exp(x1 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n","        [np.exp(x2 / x3) / (np.exp(x1 / x3) + np.exp(x2 / x3)) - (1/((x1+x2)**2))*np.exp(1/(x1 + x2))],\n","        [np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) - (x1 * np.exp(x1 / x3) + x2 * np.exp(x2 / x3)) /\n","         (x3 * (np.exp(x1 / x3) + np.exp(x2 / x3))) + 2 * (x[2] - 2)]\n","    ])\n","    return f"]},{"cell_type":"markdown","id":"c68db1dc","metadata":{"id":"c68db1dc"},"source":["Defining parameters for backtracking search:"]},{"cell_type":"code","execution_count":4,"id":"6ee422e8-763c-4063-806a-b1c5b380e7cc","metadata":{"tags":[],"id":"6ee422e8-763c-4063-806a-b1c5b380e7cc","executionInfo":{"status":"ok","timestamp":1739113121333,"user_tz":-420,"elapsed":42,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["alp = 0.4\n","beta = 0.5\n","eps = 10**(-5)"]},{"cell_type":"markdown","id":"0d1d7359","metadata":{"id":"0d1d7359"},"source":["Start Point:"]},{"cell_type":"code","execution_count":5,"id":"60de5b5e-4a39-4740-be88-5c896dd2217d","metadata":{"id":"60de5b5e-4a39-4740-be88-5c896dd2217d","executionInfo":{"status":"ok","timestamp":1739113130795,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["x_start = np.array([3,4,5])"]},{"cell_type":"markdown","id":"b945f22b","metadata":{"id":"b945f22b"},"source":["Ensuring domain:\n","\n","$ \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t $ \\\n","where, $\\Delta x = -\\nabla f(x)$"]},{"cell_type":"code","execution_count":6,"id":"1d046b67-798c-405e-9ca5-8d605d1b4cb2","metadata":{"tags":[],"id":"1d046b67-798c-405e-9ca5-8d605d1b4cb2","executionInfo":{"status":"ok","timestamp":1739113163177,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["# Ensuring Domain\n","def domain_t(x):\n","    t = 1\n","    while True:\n","        v = x - t * nabla_f(x).flatten()\n","        e3 = v[2]\n","        e2 = v[1]\n","        e1 = v[0]\n","\n","        if e3 > 0 and (e2+e1>0):\n","            return t  # Exit the loop and return 't' if the condition is met\n","\n","        # If (e3) or (e1+ e2) is negative , adjust 't' and update 'x'\n","        t *= beta\n","\n","    return None  # Return None if the condition doesn't satisfy within the maximum iterations (which can be defined)"]},{"cell_type":"markdown","id":"96676d20","metadata":{"id":"96676d20"},"source":["Backtracking algorithm:\n","\n","$\n","\\text{Given a descent direction } \\Delta x = -\\nabla f(x) \\text{ for } f \\text{ at } x \\in \\textbf{dom} f, \\alpha \\in (0, 0.5), \\beta \\in (0, 1).$\n","\n","\\begin{array}{l}\n","\\text{Set } t := 1. \\\\\n","\\text{Ensure domain:} \\; \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t \\\\\n","\\text{While } f(x + t\\Delta x) > f(x) + \\alpha t \\nabla f(x)^T \\Delta x, \\text{ set } t := \\beta t.\n","\\end{array}\n","\n"]},{"cell_type":"code","execution_count":7,"id":"6e29ee07-192b-4af4-a8cb-0e76cfaef980","metadata":{"tags":[],"id":"6e29ee07-192b-4af4-a8cb-0e76cfaef980","executionInfo":{"status":"ok","timestamp":1739114040537,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"outputs":[],"source":["# Backtracking Algorithm\n","def Backtrack_t(x):\n","    t = domain_t(x)\n","    xv = x - t * nabla_f(x).flatten()\n","    le = my_f(xv)                                              # Left expression\n","    re1 = my_f(x)\n","    re2 = np.dot(nabla_f(x).flatten(), nabla_f(x).flatten())\n","    re = re1 - alp * t * re2                                   # Right expression\n","\n","    while le > re:\n","        t *= beta\n","        xv = x - t * nabla_f(x).flatten()\n","        le = my_f(xv)\n","        re = re1 - alp * t * re2\n","    return t"]},{"cell_type":"markdown","id":"d3246df8","metadata":{"id":"d3246df8"},"source":["### Algorithm: Gradient Descent\n","\n","1. **Input:** Starting point $x$ in $\\text{dom} \\, f$\n","\n","2. **Repeat until stopping criterion is satisfied:**\n","\n","    a. $\\Delta x := -\\nabla f(x)$\n","    \n","    b. **Line search:** Choose step size $t$ via backtracking line search\n","    \n","    c. **Update:** $x := x + t \\Delta x$\n"]},{"cell_type":"code","execution_count":8,"id":"94c099d9-a67d-46fd-b3ae-aa753606c93d","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"94c099d9-a67d-46fd-b3ae-aa753606c93d","executionInfo":{"status":"ok","timestamp":1739114179391,"user_tz":-420,"elapsed":20,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}},"outputId":"e5bcafac-0ae7-40b4-9987-23c6f898dd4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal solution: [0.92618727 0.92622965 1.65342641]\n","Optimal function value: 3.908113786397637\n","Number of iterations taken to converge: 30\n"]}],"source":["# Running Gradient Descent with Backtracking Line Search\n","norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n","\n","iter =0\n","while norm_nabla_f > eps:\n","    direction = -nabla_f(x_start).flatten()\n","    t = Backtrack_t(x_start)\n","    x_start = x_start + t * direction\n","    norm_nabla_f = np.dot(nabla_f(x_start).flatten(), nabla_f(x_start).flatten())**0.5\n","    iter=iter+1\n","\n","print(\"Optimal solution:\", x_start)\n","fopt = my_f(x_start)\n","print(\"Optimal function value:\", fopt)\n","print(\"Number of iterations taken to converge:\", iter)"]},{"cell_type":"markdown","source":["\n","##  Newton's method with Backtracking Line Search:\n"],"metadata":{"id":"pGXifwZ2TKFr"},"id":"pGXifwZ2TKFr"},{"cell_type":"markdown","source":["Defining the Second Derivative:\n","\n","The gradient vector, denoted as $\\nabla f$, is the vector of partial derivatives:\n","$\n","\\nabla f(x) = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)\n","$\n","\n","The finite difference approximation for the gradient at a point $x$ is given by:\n","$\n","\\nabla_f(x) \\approx \\frac{f(x + h\\mathbf{i}) - f(x)}{h}\n","$\n","where $\\mathbf{i}$ is a unit vector along one of the coordinate axes.\n","\n","The second derivative matrix is then approximated as:\n","$\n","\\nabla^2 f(x) \\approx \\frac{1}{h} \\left(\\nabla_f(x + h\\mathbf{i}) - \\nabla_f(x)\\right)\n","$\n","\n","The reshaped second derivative matrix is a $3 \\times 3$ matrix obtained from the flattened vector."],"metadata":{"id":"VMJr5hEzUVN_"},"id":"VMJr5hEzUVN_"},{"cell_type":"code","source":["# Defing second derivative of the function\n","def nabla2_f(x):\n","    x = x.flatten()\n","    h = 1e-5  # Step size\n","    identity_matrix = np.eye(len(x))  # Identity matrix\n","\n","    # Construct the perturbation matrix with h values along the diagonal\n","    h_matrix = h * identity_matrix\n","\n","    # Calculate the forward differences for all components simultaneously\n","    perturbed_values = np.array([nabla_f(x + h_vec) for h_vec in h_matrix])\n","\n","    # Calculate the second derivative approximation\n","    second_derivative_matrix = (perturbed_values - nabla_f(x)) / h\n","\n","    reshaped_second_derivative_matrix = np.reshape(second_derivative_matrix, (3, 3))\n","\n","    return reshaped_second_derivative_matrix"],"metadata":{"id":"ntQmvv1jTJQO","executionInfo":{"status":"ok","timestamp":1739114825169,"user_tz":-420,"elapsed":39,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"id":"ntQmvv1jTJQO","execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Defining parameters for backtracking search and the start point:"],"metadata":{"id":"bH8rNteoUo7f"},"id":"bH8rNteoUo7f"},{"cell_type":"code","source":["alp = 0.4\n","beta = 0.5\n","eps = 10**(-5)\n","x_start = np.array([3,4,5])"],"metadata":{"id":"jwcir6z3UlEw","executionInfo":{"status":"ok","timestamp":1739114856963,"user_tz":-420,"elapsed":5,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"id":"jwcir6z3UlEw","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Ensuring domain:\n","\n","$ \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t $ \\\n","where\n","$\\Delta x = -\\nabla^2 f(x)^{-1} \\nabla f(x)$"],"metadata":{"id":"I9ViSH1xUu-G"},"id":"I9ViSH1xUu-G"},{"cell_type":"code","source":["# Ensuring Domain\n","def domain_t(x):\n","    t = 1\n","    while True:\n","        invdel2_f = np.linalg.inv(nabla2_f(x))      # invdel2_f.shape = (3, 3), type = numpy.ndarray\n","        del_f = nabla_f(x).flatten()\n","        direction = - np.dot(invdel2_f,del_f.flatten())             # Newton Step\n","        v = x + t * direction\n","        e3 = v[2]\n","        e2 = v[1]\n","        e1 = v[0]\n","\n","        if e3 > 0 and (e2+e1>0):\n","            return t  # Exit the loop and return 't' if the condition is met\n","\n","        # If (e3) or (e1+ e2) is negative , adjust 't' and update 'x'\n","        t *= beta\n","\n","    return None  # Return None if the condition doesn't satisfy within the maximum iterations (which can be defined)"],"metadata":{"id":"cZ2MvCS6UvXa","executionInfo":{"status":"ok","timestamp":1739115003062,"user_tz":-420,"elapsed":39,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"id":"cZ2MvCS6UvXa","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Backtracking algorithm:\n","\n","$\n","\\text{Given a descent direction } \\Delta x = -\\nabla^2 f(x)^{-1} \\nabla f(x) \\text{ for } f \\text{ at } x \\in \\textbf{dom} f, \\alpha \\in (0, 0.5), \\beta \\in (0, 1).$\n","\n","\\begin{array}{l}\n","\\text{Set } t := 1. \\\\\n","\\text{Ensure domain:} \\; \\text{While} \\; x + t\\Delta x \\notin \\textbf{dom} f, \\text{ set } t := \\beta t \\\\\n","\\text{While } f(x + t\\Delta x) > f(x) + \\alpha t \\nabla f(x)^T \\Delta x, \\text{ set } t := \\beta t.\n","\\end{array}\n"],"metadata":{"id":"QDG-al9rVUW-"},"id":"QDG-al9rVUW-"},{"cell_type":"code","source":["# Backtracking Algorithm\n","def Backtrack_t(x):\n","    t = domain_t(x)\n","    invdel2_f = np.linalg.inv(nabla2_f(x))                     # invdel2_f.shape = (3, 3), type = numpy.ndarray\n","    del_f = nabla_f(x).flatten()\n","    direction = - np.dot(invdel2_f,del_f.flatten())            # Newton Step\n","\n","    xv = x + t * direction\n","    le = my_f(xv)                                              # Left expression\n","\n","    re1 = my_f(x)\n","    decrement = np.dot(del_f.T,direction.flatten())\n","    re2 = decrement\n","    re = re1 + alp * t * re2                                   # Right expression\n","\n","    while le > re:\n","        t *= beta\n","\n","        xv = x + t * direction\n","        le = my_f(xv)\n","        re = re1 + alp * t * re2\n","    return t"],"metadata":{"id":"RSuh7WHnVU-P","executionInfo":{"status":"ok","timestamp":1739115052932,"user_tz":-420,"elapsed":45,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}}},"id":"RSuh7WHnVU-P","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["### Newton's Algorithm\n","\n","**Input:** (defined earlier)\n","- Starting point $x \\in \\text{dom} \\, f$\n","- Tolerance $\\varepsilon > 0$\n","\n","**Repeat:**\n","1. Compute the Newton step and decrement.\n","   $\n","   \\Delta x_{nt} := -\\nabla^2 f(x)^{-1}\\nabla f(x); \\quad \\lambda^2 := \\nabla f(x)^T \\nabla^2 f(x)^{-1}\\nabla f(x)\n","   $\n","2. Stopping criterion. $\\textbf{quit}$ if $\\lambda^2/2 \\leq \\varepsilon.$\n","3. Line search. Choose step size $t$ by backtracking line search; ensuring update $ := x + t \\Delta x$ lies in $\\mathbf{dom} f$ throughout.\n","4. Update. $x := x + t\\Delta x_{nt}$."],"metadata":{"id":"L5XqylZQVf03"},"id":"L5XqylZQVf03"},{"cell_type":"code","source":["# Running Newton's Algorithm\n","iter =0\n","while True:\n","    invdel2_f = np.linalg.inv(nabla2_f(x_start))            # invdel2_f.shape = (3, 3), type = numpy.ndarray\n","    del_f = nabla_f(x_start).flatten()\n","    direction = - np.dot(invdel2_f,del_f.flatten())         # Newton Step\n","\n","    decrement = - np.dot(del_f.T,direction.flatten())       # lambda^2 condition\n","\n","    if (decrement /2 <= eps):\n","        break\n","\n","    t = Backtrack_t(x_start)                                 # Choosing t using Line Search\n","\n","    x_start = x_start + t * direction                        # Update Step\n","    iter=iter+1\n","\n","print(\"Optimal solution:\", x_start)\n","fopt = my_f(x_start)\n","print(\"Optimal function value:\", fopt)\n","print(\"Number of iterations taken to converge:\", iter)"],"metadata":{"id":"WqvNURC2VgQ3","executionInfo":{"status":"ok","timestamp":1739115080974,"user_tz":-420,"elapsed":50,"user":{"displayName":"Quang Huy Đào","userId":"17473580993438237657"}},"outputId":"dda6feeb-04bb-4eaa-eeaa-b2d2c7920aba","colab":{"base_uri":"https://localhost:8080/"}},"id":"WqvNURC2VgQ3","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal solution: [0.9259803  0.92600427 1.65330163]\n","Optimal function value: 3.9081138659666275\n","Number of iterations taken to converge: 4\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}